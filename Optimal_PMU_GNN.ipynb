{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3fbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33480a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset_in, percentage):\n",
    "    data_size = len(dataset_in)\n",
    "    index = [i for i in range(len(dataset))]\n",
    "    np.random.shuffle(index)\n",
    "    dataset_random = dataset_in[index,:]\n",
    "    return dataset_random[:int(data_size*percentage/100)],dataset_random[int(data_size*percentage/100):]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        y_raw_1=[dataset[i,4800]]\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.append(list(dataset[i,200*n:200*n+200])) \n",
    "        \n",
    "        x_raw.append(list(x_raw_1))\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "\n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1643f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('train_3rd_wr.xlsx').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789ba53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  ... 0.  0.  3.5]\n"
     ]
    }
   ],
   "source": [
    "dataset_0 = np.zeros([1100,4801])\n",
    "#dataset_10b=np.concatenate((dataset[:,0:400],dataset[:,600:800],dataset[:,1000:1800],dataset[:,2000:2200],dataset[:,2400:2600],dataset[:,2600:3000],dataset[:,3200:3400],dataset[:,3600:4400],dataset[:,4600:4800],dataset[:,5000:5201]),axis=1)\n",
    "#RoCoF Features\n",
    "#dataset_0[:,0:200]=dataset[:,2600:2800] # bus 1 nodal features\n",
    "dataset_0[:,200:400]=dataset[:,2800:3000] # bus 2 nodal features\n",
    "dataset_0[:,2400:2600]=dataset[:,3200:3400] # bus 13 nodal features\n",
    "dataset_0[:,3000:3200]=dataset[:,3800:4000] # bus 16 nodal features\n",
    "#dataset_0[:,3400:3600]=dataset[:,4000:4200] # bus 18 nodal features\n",
    "dataset_0[:,4000:4200]=dataset[:,4200:4400] # bus 21 nodal features\n",
    "dataset_0[:,4400:4600]=dataset[:,4800:5000] # bus 22,23 nodal features\n",
    "dataset_0[:,4800:4801]=dataset[:,5200:5201] # labels\n",
    "print(dataset_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146976af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 5201)\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 80\n",
    "val_percentage = 20\n",
    "\n",
    "print(dataset.shape)\n",
    "train_dataset,val_dataset  = slice_dataset(dataset_0, train_percentage)\n",
    "#val_dataset = slice_dataset(dataset_0, val_percentage)\n",
    "\n",
    "n_bus = 24\n",
    "\n",
    "#actual data\n",
    "x_raw_train, y_raw_train = make_dataset(train_dataset,n_bus)\n",
    "x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8221d4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.50000],\n",
       "        [4.00000],\n",
       "        [5.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [3.00000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [6.50000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [6.00000],\n",
       "        [5.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [7.00000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [5.00000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [7.00000],\n",
       "        [6.00000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [3.50000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [5.00000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [6.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [7.00000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [7.50000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [3.00000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [4.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [3.50000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [4.00000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [7.00000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [6.50000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [6.50000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [6.00000],\n",
       "        [8.00000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [3.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [3.00000],\n",
       "        [7.00000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [6.50000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [5.50000],\n",
       "        [5.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [8.00000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [5.50000],\n",
       "        [7.00000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [3.00000],\n",
       "        [7.50000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [5.00000],\n",
       "        [7.50000],\n",
       "        [5.00000],\n",
       "        [4.50000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [6.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [6.00000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [6.00000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [4.00000],\n",
       "        [4.00000],\n",
       "        [3.00000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [4.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [6.00000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [3.00000],\n",
       "        [5.50000],\n",
       "        [4.00000],\n",
       "        [6.50000],\n",
       "        [4.00000],\n",
       "        [4.50000],\n",
       "        [3.50000],\n",
       "        [7.00000],\n",
       "        [6.00000],\n",
       "        [3.00000],\n",
       "        [8.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [5.50000],\n",
       "        [4.50000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [8.00000],\n",
       "        [7.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [4.50000],\n",
       "        [7.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [3.00000],\n",
       "        [6.50000],\n",
       "        [3.50000],\n",
       "        [4.00000],\n",
       "        [3.50000],\n",
       "        [7.50000],\n",
       "        [4.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [7.00000],\n",
       "        [7.00000],\n",
       "        [7.50000],\n",
       "        [5.50000],\n",
       "        [7.50000],\n",
       "        [8.00000],\n",
       "        [6.50000],\n",
       "        [5.50000],\n",
       "        [7.00000],\n",
       "        [4.50000],\n",
       "        [5.50000],\n",
       "        [5.00000],\n",
       "        [5.00000],\n",
       "        [3.50000],\n",
       "        [6.50000],\n",
       "        [5.00000],\n",
       "        [7.00000],\n",
       "        [3.50000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a07ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\python38\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = x_raw_train, y_raw_train\n",
    "x_val, y_val = x_raw_val, y_raw_val\n",
    "edge_index = torch.tensor([[0, 1, 0, 2, 0, 4, 1, 3, 1, 5, 2, 8, 2,23, 3, 8, 4, 9, 5, 9, 6, 7, 7, 8, 7, 9, 8,10, 8,11, 9,10, 9,11,10,12,10,13,11,12,11,22,12,22,13,15,14,15,14,20,14,23,15,16,15,18,16,17,16,21,17,20,18,19,19,22,20,21],\n",
    "                           [1, 0, 2, 0, 4, 0, 3, 1, 5, 1, 8, 2,23, 2, 8, 3, 9, 4, 9, 5, 7, 6, 8, 7, 9, 7,10, 8,11, 8,10, 9,11, 9,12,10,13,10,12,11,22,11,22,12,15,13,15,14,20,14,23,14,16,15,18,15,17,16,21,16,20,17,19,18,22,19,21,20]], dtype=torch.long)\n",
    "\n",
    "data_train_list, data_val_list = [], []\n",
    "for i,_ in enumerate(x_train):\n",
    "    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))\n",
    "for i,_ in enumerate(x_val):\n",
    "    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))\n",
    "\n",
    "train_loader = DataLoader(data_train_list, batch_size=1)\n",
    "val_loader = DataLoader(data_val_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440de0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_GNN_NN(torch.nn.Module):\n",
    "    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):\n",
    "        super(My_GNN_NN, self).__init__()\n",
    "        self.feat_in = feat_in if feat_in is not None else 200\n",
    "        self.feat_size1 = feat_in if feat_in is not None else 64\n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 64\n",
    "        self.output_size = output_size if output_size is not None else 1\n",
    "        \n",
    "        self.conv1 = GCNConv(feat_in, feat_size1)\n",
    "        self.lin1 = Linear(node_size*feat_size1, hidden_size1)\n",
    "        self.lin2 = Linear(hidden_size1, output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = x.flatten(start_dim = 0)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e4e565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias\n",
      "torch.Size([64])\n",
      "conv1.lin.weight\n",
      "torch.Size([64, 200])\n",
      "lin1.weight\n",
      "torch.Size([64, 1536])\n",
      "lin1.bias\n",
      "torch.Size([64])\n",
      "lin2.weight\n",
      "torch.Size([1, 64])\n",
      "lin2.bias\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_in = 200\n",
    "feat_size1 = 64\n",
    "hidden_size1 = 64\n",
    "output_size = 1\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)\n",
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68621c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0    train loss: 2.8204839    val loss: 2.5021915\n",
      "epoch: 10    train loss: 2.6027925    val loss: 2.4999095\n",
      "epoch: 20    train loss: 0.0586028    val loss: 0.0812420\n",
      "epoch: 30    train loss: 0.0303462    val loss: 0.0547229\n",
      "epoch: 40    train loss: 0.0171581    val loss: 0.0477991\n",
      "epoch: 50    train loss: 0.0107767    val loss: 0.0402777\n",
      "epoch: 60    train loss: 0.0070486    val loss: 0.0371322\n",
      "epoch: 70    train loss: 0.0051389    val loss: 0.0265033\n",
      "epoch: 80    train loss: 0.0056159    val loss: 0.0361994\n",
      "epoch: 90    train loss: 0.0212294    val loss: 0.0265386\n",
      "epoch: 100    train loss: 0.0017746    val loss: 0.0373461\n",
      "epoch: 110    train loss: 0.0034988    val loss: 0.0316771\n",
      "epoch: 120    train loss: 0.0066225    val loss: 0.0312897\n",
      "epoch: 130    train loss: 0.0049163    val loss: 0.0314205\n",
      "epoch: 140    train loss: 0.0099325    val loss: 0.0303056\n",
      "epoch: 150    train loss: 0.0010673    val loss: 0.0360982\n",
      "epoch: 160    train loss: 0.0022973    val loss: 0.0313410\n",
      "epoch: 170    train loss: 0.0101224    val loss: 0.0364975\n",
      "epoch: 180    train loss: 0.0040714    val loss: 0.0313026\n",
      "epoch: 190    train loss: 0.0032048    val loss: 0.0522864\n",
      "epoch: 200    train loss: 0.0063857    val loss: 0.0371504\n",
      "epoch: 210    train loss: 0.0085185    val loss: 0.0290179\n",
      "epoch: 220    train loss: 0.0025770    val loss: 0.0413926\n",
      "epoch: 230    train loss: 0.0014508    val loss: 0.0352497\n",
      "epoch: 240    train loss: 0.0023889    val loss: 0.0426037\n",
      "epoch: 250    train loss: 0.0012832    val loss: 0.0370977\n",
      "epoch: 260    train loss: 0.0010251    val loss: 0.0472721\n",
      "epoch: 270    train loss: 0.0040961    val loss: 0.0588695\n",
      "epoch: 280    train loss: 0.0067677    val loss: 0.0440285\n",
      "epoch: 290    train loss: 0.0008462    val loss: 0.0431212\n",
      "epoch: 300    train loss: 0.0024860    val loss: 0.0333929\n",
      "epoch: 310    train loss: 0.0109088    val loss: 0.0436661\n",
      "epoch: 320    train loss: 0.0011194    val loss: 0.0380688\n",
      "epoch: 330    train loss: 0.0007974    val loss: 0.0379231\n",
      "epoch: 340    train loss: 0.0013692    val loss: 0.0381602\n",
      "epoch: 350    train loss: 0.0012272    val loss: 0.0506123\n",
      "epoch: 360    train loss: 0.0036676    val loss: 0.0380449\n",
      "epoch: 370    train loss: 0.0151937    val loss: 0.0493543\n",
      "epoch: 380    train loss: 0.0035024    val loss: 0.0410061\n",
      "epoch: 390    train loss: 0.0012612    val loss: 0.0386972\n",
      "epoch: 400    train loss: 0.0109397    val loss: 0.0511132\n",
      "epoch: 410    train loss: 0.0014965    val loss: 0.0331277\n",
      "epoch: 420    train loss: 0.0062669    val loss: 0.0519196\n",
      "epoch: 430    train loss: 0.0010146    val loss: 0.0329324\n",
      "epoch: 440    train loss: 0.0008013    val loss: 0.0481543\n",
      "epoch: 450    train loss: 0.0007113    val loss: 0.0324015\n",
      "epoch: 460    train loss: 0.0006817    val loss: 0.0435066\n",
      "epoch: 470    train loss: 0.0039803    val loss: 0.0324708\n",
      "epoch: 480    train loss: 0.0108239    val loss: 0.0375927\n",
      "epoch: 490    train loss: 0.0011850    val loss: 0.0322321\n",
      "epoch: 500    train loss: 0.0026468    val loss: 0.0450349\n",
      "epoch: 510    train loss: 0.0008024    val loss: 0.0306094\n",
      "epoch: 520    train loss: 0.0045477    val loss: 0.0331211\n",
      "epoch: 530    train loss: 0.0004770    val loss: 0.0360289\n",
      "epoch: 540    train loss: 0.0006452    val loss: 0.0320237\n",
      "epoch: 550    train loss: 0.0018309    val loss: 0.0317188\n",
      "epoch: 560    train loss: 0.0006996    val loss: 0.0375718\n",
      "epoch: 570    train loss: 0.0002138    val loss: 0.0391023\n",
      "epoch: 580    train loss: 0.0001210    val loss: 0.0399152\n",
      "epoch: 590    train loss: 0.0001264    val loss: 0.0369534\n",
      "epoch: 600    train loss: 0.0037421    val loss: 0.0314724\n",
      "epoch: 610    train loss: 0.0056786    val loss: 0.0314171\n",
      "epoch: 620    train loss: 0.0026102    val loss: 0.0318322\n",
      "epoch: 630    train loss: 0.0005451    val loss: 0.0266117\n",
      "epoch: 640    train loss: 0.0066010    val loss: 0.0374409\n",
      "epoch: 650    train loss: 0.0002846    val loss: 0.0344358\n",
      "epoch: 660    train loss: 0.0004200    val loss: 0.0411820\n",
      "epoch: 670    train loss: 0.0000448    val loss: 0.0342143\n",
      "epoch: 680    train loss: 0.0003698    val loss: 0.0273805\n",
      "epoch: 690    train loss: 0.0001148    val loss: 0.0398667\n",
      "epoch: 700    train loss: 0.0025919    val loss: 0.0331363\n",
      "epoch: 710    train loss: 0.0070155    val loss: 0.0351093\n",
      "epoch: 720    train loss: 0.0000263    val loss: 0.0290751\n",
      "epoch: 730    train loss: 0.0193026    val loss: 0.0537568\n",
      "epoch: 740    train loss: 0.0089773    val loss: 0.0435361\n",
      "epoch: 750    train loss: 0.0099389    val loss: 0.0359266\n",
      "epoch: 760    train loss: 0.0078280    val loss: 0.0290583\n",
      "epoch: 770    train loss: 0.0028077    val loss: 0.0324812\n",
      "epoch: 780    train loss: 0.0027755    val loss: 0.0552863\n",
      "epoch: 790    train loss: 0.0000405    val loss: 0.0273585\n",
      "epoch: 800    train loss: 0.0009456    val loss: 0.0322103\n",
      "epoch: 810    train loss: 0.0004152    val loss: 0.0399556\n",
      "epoch: 820    train loss: 0.0000112    val loss: 0.0305452\n",
      "epoch: 830    train loss: 0.0002501    val loss: 0.0400905\n",
      "epoch: 840    train loss: 0.0001214    val loss: 0.0348562\n",
      "epoch: 850    train loss: 0.0000864    val loss: 0.0439329\n",
      "epoch: 860    train loss: 0.0047304    val loss: 0.0401991\n",
      "epoch: 870    train loss: 0.0001400    val loss: 0.0333055\n",
      "epoch: 880    train loss: 0.0008523    val loss: 0.0318004\n",
      "epoch: 890    train loss: 0.0002672    val loss: 0.0274789\n",
      "epoch: 900    train loss: 0.0000410    val loss: 0.0382279\n",
      "epoch: 910    train loss: 0.0088533    val loss: 0.0371083\n",
      "epoch: 920    train loss: 0.0261870    val loss: 0.0404553\n",
      "epoch: 930    train loss: 0.0071262    val loss: 0.0307366\n",
      "epoch: 940    train loss: 0.0045984    val loss: 0.0351541\n",
      "epoch: 950    train loss: 0.0010512    val loss: 0.0354224\n",
      "epoch: 960    train loss: 0.0036766    val loss: 0.0237918\n",
      "epoch: 970    train loss: 0.0000432    val loss: 0.0363936\n",
      "epoch: 980    train loss: 0.0002985    val loss: 0.0271153\n",
      "epoch: 990    train loss: 0.0003561    val loss: 0.0287956\n",
      "epoch: 1000    train loss: 0.0002604    val loss: 0.0284486\n",
      "epoch: 1010    train loss: 0.0000477    val loss: 0.0387222\n",
      "epoch: 1020    train loss: 0.0000025    val loss: 0.0369003\n",
      "epoch: 1030    train loss: 0.0001075    val loss: 0.0383158\n",
      "epoch: 1040    train loss: 0.0002165    val loss: 0.0375764\n",
      "epoch: 1050    train loss: 0.0432297    val loss: 0.0653164\n",
      "epoch: 1060    train loss: 0.0021190    val loss: 0.0437225\n",
      "epoch: 1070    train loss: 0.0001735    val loss: 0.0452765\n",
      "epoch: 1080    train loss: 0.0002074    val loss: 0.0347151\n",
      "epoch: 1090    train loss: 0.0171496    val loss: 0.0553542\n",
      "epoch: 1100    train loss: 0.0041708    val loss: 0.0411296\n",
      "epoch: 1110    train loss: 0.0045706    val loss: 0.0413527\n",
      "epoch: 1120    train loss: 0.0090765    val loss: 0.0366269\n",
      "epoch: 1130    train loss: 0.0070226    val loss: 0.0370029\n",
      "epoch: 1140    train loss: 0.0033847    val loss: 0.0463323\n",
      "epoch: 1150    train loss: 0.0017117    val loss: 0.0379301\n",
      "epoch: 1160    train loss: 0.0027736    val loss: 0.0447492\n",
      "epoch: 1170    train loss: 0.0031029    val loss: 0.0510611\n",
      "epoch: 1180    train loss: 0.0017247    val loss: 0.0332245\n",
      "epoch: 1190    train loss: 0.0049105    val loss: 0.0400276\n",
      "epoch: 1200    train loss: 0.0019933    val loss: 0.0389995\n",
      "epoch: 1210    train loss: 0.0020808    val loss: 0.0341714\n",
      "epoch: 1220    train loss: 0.0014268    val loss: 0.0475177\n",
      "epoch: 1230    train loss: 0.0011037    val loss: 0.0441972\n",
      "epoch: 1240    train loss: 0.0017024    val loss: 0.0449507\n",
      "epoch: 1250    train loss: 0.0030308    val loss: 0.0432820\n",
      "epoch: 1260    train loss: 0.0009537    val loss: 0.0321900\n",
      "epoch: 1270    train loss: 0.0016661    val loss: 0.0345470\n",
      "epoch: 1280    train loss: 0.0028348    val loss: 0.0374160\n",
      "epoch: 1290    train loss: 0.0022709    val loss: 0.0458236\n",
      "epoch: 1300    train loss: 0.0014852    val loss: 0.0375223\n",
      "epoch: 1310    train loss: 0.0035624    val loss: 0.0360271\n",
      "epoch: 1320    train loss: 0.0025059    val loss: 0.0358793\n",
      "epoch: 1330    train loss: 0.0126793    val loss: 0.0492320\n",
      "epoch: 1340    train loss: 0.0018399    val loss: 0.0410003\n",
      "epoch: 1350    train loss: 0.0092947    val loss: 0.0421205\n",
      "epoch: 1360    train loss: 0.0055344    val loss: 0.0434784\n",
      "epoch: 1370    train loss: 0.0071310    val loss: 0.0400558\n",
      "epoch: 1380    train loss: 0.0057745    val loss: 0.0423006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1390    train loss: 0.0021931    val loss: 0.0418801\n",
      "epoch: 1400    train loss: 0.0115255    val loss: 0.0755107\n",
      "epoch: 1410    train loss: 0.0018853    val loss: 0.0370198\n",
      "epoch: 1420    train loss: 0.0053852    val loss: 0.0317300\n",
      "epoch: 1430    train loss: 0.0005507    val loss: 0.0347071\n",
      "epoch: 1440    train loss: 0.0005936    val loss: 0.0342895\n",
      "epoch: 1450    train loss: 0.0064479    val loss: 0.0346722\n",
      "epoch: 1460    train loss: 0.0006895    val loss: 0.0336098\n",
      "epoch: 1470    train loss: 0.0064589    val loss: 0.0436805\n",
      "epoch: 1480    train loss: 0.0012372    val loss: 0.0373359\n",
      "epoch: 1490    train loss: 0.0006957    val loss: 0.0333354\n",
      "epoch: 1500    train loss: 0.0015283    val loss: 0.0312483\n",
      "epoch: 1510    train loss: 0.0073718    val loss: 0.0431883\n",
      "epoch: 1520    train loss: 0.0063426    val loss: 0.0550247\n",
      "epoch: 1530    train loss: 0.0015720    val loss: 0.0379481\n",
      "epoch: 1540    train loss: 0.0020319    val loss: 0.0378780\n",
      "epoch: 1550    train loss: 0.0034998    val loss: 0.0341061\n",
      "epoch: 1560    train loss: 0.0005057    val loss: 0.0381449\n",
      "epoch: 1570    train loss: 0.0061291    val loss: 0.0397581\n",
      "epoch: 1580    train loss: 0.0009156    val loss: 0.0314190\n",
      "epoch: 1590    train loss: 0.0007222    val loss: 0.0387550\n",
      "epoch: 1600    train loss: 0.0096479    val loss: 0.0437466\n",
      "epoch: 1610    train loss: 0.0020328    val loss: 0.0363309\n",
      "epoch: 1620    train loss: 0.0076078    val loss: 0.0582815\n",
      "epoch: 1630    train loss: 0.0004200    val loss: 0.0308864\n",
      "epoch: 1640    train loss: 0.0014704    val loss: 0.0344886\n",
      "epoch: 1650    train loss: 0.0001943    val loss: 0.0304515\n",
      "epoch: 1660    train loss: 0.0000708    val loss: 0.0348485\n",
      "epoch: 1670    train loss: 0.0000273    val loss: 0.0321448\n",
      "epoch: 1680    train loss: 0.0000811    val loss: 0.0315854\n",
      "epoch: 1690    train loss: 0.0003492    val loss: 0.0380698\n",
      "epoch: 1700    train loss: 0.0000040    val loss: 0.0352825\n",
      "epoch: 1710    train loss: 0.0009715    val loss: 0.0447937\n",
      "epoch: 1720    train loss: 0.0018131    val loss: 0.0270683\n",
      "epoch: 1730    train loss: 0.0000326    val loss: 0.0359573\n",
      "epoch: 1740    train loss: 0.0003051    val loss: 0.0368813\n",
      "epoch: 1750    train loss: 0.0003440    val loss: 0.0327047\n",
      "epoch: 1760    train loss: 0.0000098    val loss: 0.0341377\n",
      "epoch: 1770    train loss: 0.0000934    val loss: 0.0316378\n",
      "epoch: 1780    train loss: 0.0004904    val loss: 0.0358180\n",
      "epoch: 1790    train loss: 0.0049286    val loss: 0.0365574\n",
      "epoch: 1800    train loss: 0.0006394    val loss: 0.0461440\n",
      "epoch: 1810    train loss: 0.0000292    val loss: 0.0321340\n",
      "epoch: 1820    train loss: 0.0002084    val loss: 0.0325620\n",
      "epoch: 1830    train loss: 0.0003027    val loss: 0.0359467\n",
      "epoch: 1840    train loss: 0.0003532    val loss: 0.0486957\n",
      "epoch: 1850    train loss: 0.0001733    val loss: 0.0327701\n",
      "epoch: 1860    train loss: 0.0001792    val loss: 0.0402565\n",
      "epoch: 1870    train loss: 0.0045003    val loss: 0.0305103\n",
      "epoch: 1880    train loss: 0.0001512    val loss: 0.0306031\n",
      "epoch: 1890    train loss: 0.0001583    val loss: 0.0356160\n",
      "epoch: 1900    train loss: 0.0002036    val loss: 0.0261863\n",
      "epoch: 1910    train loss: 0.0000065    val loss: 0.0341957\n",
      "epoch: 1920    train loss: 0.0001017    val loss: 0.0329297\n",
      "epoch: 1930    train loss: 0.0034554    val loss: 0.0289929\n",
      "epoch: 1940    train loss: 0.0015143    val loss: 0.0334350\n",
      "epoch: 1950    train loss: 0.0045170    val loss: 0.0313552\n",
      "epoch: 1960    train loss: 0.0009752    val loss: 0.0263258\n",
      "epoch: 1970    train loss: 0.0000987    val loss: 0.0309317\n",
      "epoch: 1980    train loss: 0.0001373    val loss: 0.0341034\n",
      "epoch: 1990    train loss: 0.0007286    val loss: 0.0285060\n",
      "epoch: 2000    train loss: 0.0016782    val loss: 0.0267332\n"
     ]
    }
   ],
   "source": [
    "feat_in = 200\n",
    "feat_size1 = 64\n",
    "hidden_size1 = 64\n",
    "output_size =1\n",
    "lr = 0.001\n",
    "\n",
    "model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "count=0\n",
    "patience=2000\n",
    "lossMin = 1e10\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(2001):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_train_prediction = model(batch)\n",
    "        loss = MSE(y_train_prediction, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    for batch in val_loader:\n",
    "        y_val_prediction = model(batch)\n",
    "        loss = MSE(y_val_prediction, batch.y)\n",
    "        val_loss += loss.item() * batch.num_graphs\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    #early stopping\n",
    "    if (val_loss < lossMin):\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weights(model, \"Inertia_GNN_NN_model.pt\")\n",
    "    else:\n",
    "        count+=1\n",
    "        if(count>patience):\n",
    "            print(\"early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(best_epoch, best_train_loss, best_val_loss))\n",
    "            break\n",
    "    \n",
    "    if (train_loss <= 0):\n",
    "        print(\"min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "        break\n",
    "\n",
    "    if (epoch % 10) == 0:\n",
    "        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b953e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 2000, train loss: 0.0016782, val loss: 0.0267332\n",
      "best epoch: 1989, train loss: 0.0006502, val loss: 0.0212564\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.title('GNN NN on power flow dataset')\n",
    "plt.plot(train_loss_list, label=\"train\")\n",
    "plt.plot(val_loss_list, label=\"validation\")\n",
    "#plt.yscale('log')\n",
    "#plt.xlabel(\"Epoch\")\n",
    "#plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "#plt.savefig('GNN_training.png',dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))\n",
    "print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_train_prediction_1 = model(train_loader.dataset[0])\n",
    "train_loss_1 = MSE(y_train_prediction_1, y_raw_train[0])\n",
    "print(\"[1 datapoint] Train output ground-truth: \\n\" + str(y_raw_train[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Train output prediction: \\n\" + str(y_train_prediction_1.detach().numpy()))\n",
    "print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))\n",
    "\n",
    "train_loss = 0\n",
    "for batch in train_loader:\n",
    "    pred = model(batch)\n",
    "    loss = MSE(pred, batch.y)\n",
    "    train_loss += loss.item() * batch.num_graphs\n",
    "train_loss /= len(train_loader.dataset)\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "y_val_prediction_1 = model(val_loader.dataset[0])\n",
    "val_loss_1 = MSE(y_val_prediction_1, y_raw_train[0])\n",
    "print(\"[1 datapoint] Val output ground-truth: \\n\" + str(y_raw_val[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Val output prediction: \\n\" + str(y_val_prediction_1.detach().numpy()))\n",
    "print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6f5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
